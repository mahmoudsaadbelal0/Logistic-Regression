def Logistic_regression(X,y,n,alpha,gamma):
    m = X.shape[0]
    b=0
    y=y.reshape(-1,1)
    if gamma >0:
        thetas = np.random.rand(X.shape[1]).reshape(-1,1)
        for i in range (0,n):
            hypo = 1/(1+np.exp(-1*(np.dot(X,thetas)+b)))
            loss =hypo - y        
            derivative= (alpha/m)*np.dot(X.T,loss)
            b = b - derivative
            thetas = thetas*(1-alpha/m) - derivative        
    else:
        X1 = np.ones(X.shape[0]).reshape(-1,1)
        X=np.append(X1,X,axis=1)
        thetas = np.random.randn(X.shape[1]).reshape(-1,1)*.001
        Cost=[]
        for i in range (0,n):
            hypo = 1/(1+np.exp(-1*(np.dot(X,thetas)+b)))
            loss =hypo - y
            derivative= (alpha/m)*np.dot(X.T,loss)
            thetas = thetas - derivative
    if gamma>0:
        print('intersection point :'+ str(b),thetas)
    else:
        print(thetas)
    Cost = (1/(2*m))*(np.sum(loss**2)+gamma*np.sum(thetas**2))
    return Cost
    
    
    logistic regression using for loops
    
    def logisticRegg(X,y,n,alpha):
    W = np.zeros(X.shape[1]).reshape(-1,1)
    b = 0
    y=y.reshape(-1,1)
    for i in range(0,n):
        hypo = 1/(1+np.exp(-1*(np.dot(X,W)+b)))
        dz =hypo - y
        dw = 0
        Acumulator = []
        for i,j in enumerate(W):
            Acumulator.append(np.sum(X[:,i].reshape(-1,1)*dz))
        dw = (1.0/m)*(np.array(Acumulator).reshape(-1,1))
        db = (1.0/m)*np.sum(dz)
        W = W-alpha*dw
        b = b-alpha*db
    
    return W,b
    
many questions i have here after many iterations it come to be true but it always chooses the hardest path
    
    
    
    
